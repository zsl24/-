# 语音转换算法汇总
语音转换算法要解决的问题是，在不改变语句信息的情况下，改变一段语音的说话人身份。  
目前，主流的语音转化算法如下：
- 端到端的梅尔语谱图转换
- 语音识别模块+语音合成模块的组合，语音识别输出一个瓶颈特征(BN feature，只包含语句信息)，语音合成模块通过BN feature生成转换后的音频，两个模型是分开训练的  
- 编码-解码结构，类似seq2seq，编码器负责语音识别，解码器负责语音转换，编码器和解码器是联合训练的

## 平行语料
`平行语音数据`：平行语音数据方法的VC要求两段音频在时间上对齐，语言内容也要一致。
`非平行语音数据`：既不需要时间对齐，又不需要语言内容一致。
非平行语音数据目前有两大方法：
- Phonetic posteriorgram  
- End-to-end neural approach


## 端到端的梅尔语谱图转换
参考论文：[MelGAN-VC: Voice Conversion and Audio Style Transfer on arbitrarily long samples using Spectrograms](https://arxiv.org/abs/1910.03713)  
来源：ArXiv的预印本
发表时间：2019.12.5


## Recognition-Synthesis 模型
参考论文: [Accent and Speaker Disentanglement in Many-to-many Voice Conversion](https://arxiv.org/abs/2011.08609)  
来源：ISCSLP2021
发表时间：2020.11.17
这篇文献要解决的问题是实现语音转换+口音转换。一段语音信号可以分成三个信息：人物身份信息（人的说话特征），口音信息以及语言信息。
模型的推理过程如下：
>给定source speaker的一段语音，首先通过speech recognition以bottleneck feature的形式提取出语音信息（尽可能地去除人物身份信息），然后将带有语言信息的BN feature送入转换模型(conversion model)当中，得到target speaker的梅尔语谱图。最后，通过一个名为melLPCnet的声码器，得到target speaker语音的波形。  

很显然，recognition-synthesis模型可以分为两大块：`recognition`和`synthesis`。实际上，recognition相当于一个well-trained的前端模型，训练synthesis部分的时候，recognition的参数并不更新。

### Recognition
语音识别器的输入为`source speaker`的语音，输出为`BN feature`。  
这篇文献中使用的语音识别模块是基于口音的语音识别，即不同的口音使用的模型是不一样的。如果使用了一个general的语音识别器，那么其输出的BN feature可能就会混杂着一些非语言信息，比如口音信息。但我们要求`BN feature只包含纯净的语言信息`。所以，对不同口音的语音使用相应的语音识别器，BN feature中就不会包含相应的口音信息了。文中使用了两种基于不同口音的语音识别器，一个是标准普通话，另一个是天津普通话。  


### Synthesis (Conversion model)
转换模型的输入为`BN feature`，`target speaker ID`以及`accent ID`，输出为`target speaker 以 accent ID的口音说话的语音`  
转换模型的网络结构如下：  
![image](https://user-images.githubusercontent.com/40049927/133569423-8678e0d9-8979-4591-9855-16baa7148581.png)

### Speaker ID Embedding 和 Accent ID Embedding

### Ablation Study
文中比较了三个模型：
- Baseline：标准普通话语音识别器+转换模型，没有说话者分类器作为判别器
- P1：基于口音的语音识别器+转换模型，没有说话者分类器作为判别器
- P2：基于口音的语音识别器+转换模型，有说话者分类器


## 编码-解码 seq2seq 模型
参考论文：[Non-Parallel Sequence-to-Sequence Voice Conversion with Disentangled Linguistic and Speaker Representations](https://arxiv.org/abs/1906.10508)  
来源：IEEE/ACM Transactions on Audio, Speech and Language Processing vol 28 no 1 (2020) 540-552  


