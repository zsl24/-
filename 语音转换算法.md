# 语音转换算法汇总
语音转换算法要解决的问题是，在不改变语句信息的情况下，改变一段语音的说话人身份。  
目前，主流的语音转化算法如下：
- 端到端的梅尔语谱图转换
- 独立的语音识别模块+独立的语音合成模块的组合 
- 编码-解码结构，类似seq2seq，编码器负责语音识别，解码器负责语音合成

## 1 Voice Conversion 的应用

### 安全
一个人的声音会可能会包含个人身份信息。处于安全考虑，我们可以采用变声技术来保护自己的个人隐私不被侵犯。在视频会议中，用户由于信息安全原因或者个人原因，想要以另一个人的声音进行发言，实时语音转换技术能够将用户的声音转化为特定人物的声音，然后将转换后的声音传送到会议中的每个成员。 

### 教育：语言学习
当我们学习说一门外语，我们一般是通过听老师怎么读，然后我们进行模仿。那么如果老师通过变声技术用我们自己的声音说出例句，我们能够更好的模仿。  

### 语音助手
提供个性化的人物模板。例如自己的亲朋好友的声音，都可以作为语音转换的对象。  

### 数据增强
我们可以应用语音转换技术来生成一些我们收集不到的语料，增加数据集规模。  


## 2 平行语料
`平行语音数据`：平行语音数据方法的VC要求两段音频在时间上对齐，语言内容也要一致。  
`非平行语音数据`：既不需要时间对齐，又不需要语言内容一致。   
非平行语音数据目前有两大方法：
- `Feature Disentangle`: 语音信号可以分成不同领域的信息，说话人信息，口音信息，语句信息，通过调换这些信息达到语音转换的效果  
![image](https://user-images.githubusercontent.com/40049927/135721179-bbf61f10-36ec-46b0-98e1-8b65db06d9a6.png)
- `Direct Transformation`

## 3 特征解耦 Feature Disentangle

`Content Encoder` 将语句信息从语音信号中解耦出来，我们可以得到一个语句相关的特征图/特征向量。  
`Speaker Encoder` 将人物的身份信息从语音信号中解耦出来，我们可以得到一个人物身份特征向量。  
将语句特征向量和说话人特征向量作为输入，输入到解码器当中，最后解码器输出指定说话人关于给定语句的一段语音。

### 3.1 如何训练`Content Encoder`，`Speaker Encoder`

#### `Content Encoder`
**我们如何引导Content Encoder去正确地解耦语言信息？**  
![1633770929(1)](https://user-images.githubusercontent.com/40049927/136652272-67da585c-e035-4886-9b55-c2865f20812c.png)  
类似于AutoEncoder，输入一段语音，通过encoder编码以及decoder解码，得到一段输出的语音，优化的目标是让输入和输出尽可能接近。这样一个模型的问题在于，网络可以直接复制输入语音直接输出，可能进行无效训练。如何避免这样的问题？  
如何训练content encoder从而使得提取出的语句信息不掺杂有说话人特征信息？  
解决方法1：`Adversarial Training`  
> 训练一个discriminator，discriminator是一个说话人分类器，其工作是判断给定向量来自哪个语者。`Content Encoder`的人物就是想办法输出一些能够“糊弄”discriminator的向量，使得discriminator无法判断`Content Encoder`输出的向量是来自哪个说话人的。因此，`Content Encoder`在训练的过程中，会有意地删去说话人身份信息，保留语句信息，使得discriminator混淆。  

解决方法2：`Instance Normalization` + `Adaptive Instance Normalization`
`Instance Normalization` 可以去掉说话人信息。  
![image](https://user-images.githubusercontent.com/40049927/136927274-213867c7-ac47-41a4-89a1-1edf2a75bd8b.png)  
对于一段语音信号，
`Adaptive Instance Normalization`：在合成声音时，对说话者特征向量进行`AdaIN`操作，使得该特征向量只会影响输出语音的声音特征。  

#### `Speaker Encoder`
**如何提供有效的语者特征向量？**  
1. 去掉`Speaker Encoder`，假定语者身份已知，以`独热向量`作为语者特征向量。由于已经提供了语者身份，Decoder就不需要语者信息，通过训练优化，则Content Encoder可以忽略语者身份信息。
![1633771226(1)](https://user-images.githubusercontent.com/40049927/136652411-d2cb7956-3ca4-413e-bfea-77d2669755b1.png)  
**缺点**：
- 无法保证`Content Encoder`能够纯净地解耦出语句信息，可能还会有语者信息掺杂在其中。**但是只要确保特征向量的维度在较低的水平，也能够得到比较可观的效果**  
- 如果要引入新的目标语者，我们需要改变独热向量的尺寸，需要重新训练，很麻烦。  
2. Pre-training Encoders
我们可以使用一些现成的`Speaker Encoder`，这里涉及到Speaker Embedding技术，包括`i-vecotor`,`d-vecotor`,`x-vecotor`。


## 4 端到端的梅尔语谱图转换 (one-to-one)
参考论文：[MelGAN-VC: Voice Conversion and Audio Style Transfer on arbitrarily long samples using Spectrograms](https://arxiv.org/abs/1910.03713)  
来源：ArXiv的预印本  
发表时间：2019.12.5  


## 5 基于Phonetic Posteriorgram 模型 (many-to-one)
参考论文：[Phonetic PosteriorGrams for Many-to--one Voice Conversion without Parallel Data Training](https://www1.se.cuhk.edu.hk/~hccl/publications/pub/2016_paper_297.pdf)  
来源：2016 IEEE International Conference on Multimedia and Expo (ICME)  
会议时间：2016.7.11  

`MFCC` Mel-frequency cepstral coefficients  
`MCEPs` Mel-cepstral Coefficients  
`AP` Aperiodic Component  
`F0` Fundamental Frequency  

### Phonetic PosteriorGrams PPGs
`PPG` 横坐标是时间，纵坐标代表音素类型，图中的值代表了当前时间帧为当前音素类型的后验概率。  
![image](https://user-images.githubusercontent.com/40049927/134777461-a31eafa6-743f-4cee-afcd-a33085bca609.png)  

## 6 编码-解码 seq2seq 模型 (many-to-many)
参考论文：[Non-Parallel Sequence-to-Sequence Voice Conversion with Disentangled Linguistic and Speaker Representations](https://arxiv.org/abs/1906.10508)  
来源：IEEE/ACM Transactions on Audio, Speech and Language Processing vol 28 no 1 (2020) 540-552  
发表时间：2019.12.13

## 7 Recognition-Synthesis 模型 (many-to-many)
参考论文: [Accent and Speaker Disentanglement in Many-to-many Voice Conversion](https://arxiv.org/abs/2011.08609)  
来源：ISCSLP2021  
发表时间：2020.11.17  
这篇文献要解决的问题是实现语音转换+口音转换。一段语音信号可以分成三个信息：人物身份信息（人的说话特征），口音信息以及语言信息。
模型的推理过程如下：
>给定source speaker的一段语音，首先通过speech recognition以bottleneck feature的形式提取出语音信息（尽可能地去除人物身份信息），然后将带有语言信息的BN feature送入转换模型(conversion model)当中，得到target speaker的梅尔语谱图。最后，通过一个名为melLPCnet的声码器，得到target speaker语音的波形。  
很显然，recognition-synthesis模型可以分为两大块：`recognition`和`synthesis`。实际上，recognition相当于一个well-trained的独立前端模型，训练synthesis部分的时候，recognition的参数并不更新。

### Recognition
语音识别器的输入为`source speaker`的语音，输出为`BN feature`。  
这篇文献中使用的语音识别模块是基于口音的语音识别，即不同的口音使用的模型是不一样的。如果使用了一个general的语音识别器，那么其输出的BN feature可能就会混杂着一些非语言信息，比如口音信息。但我们要求`BN feature只包含纯净的语言信息`。所以，对不同口音的语音使用相应的语音识别器，BN feature中就不会包含相应的口音信息了。文中使用了两种基于不同口音的语音识别器，一个是标准普通话，另一个是天津普通话。  
文中使用的语音识别模块是一个TDNN-F的模型，在Kaldi toolkit用30000个小时的普通话训练数据训练。  

#### Bottleneck Feature
BN Feature可以看作是一个语言信息的表征。它是一个256维的向量，从最后的全连接网络层中获取。

### Synthesis (Conversion model)
转换模型的输入为`BN feature`，`speaker ID`以及`accent ID`，输出为`target speaker 以 accent ID的口音说话的语音`  

模型又可以分成一个`Encoder`和一个`Decoder`
![image](https://user-images.githubusercontent.com/40049927/134775485-99e82cd9-6dd1-443a-bc1b-549877f9079e.png)  

转换模型的网络结构如下：  
![image](https://user-images.githubusercontent.com/40049927/133569423-8678e0d9-8979-4591-9855-16baa7148581.png)  


#### Encoder
- CBHG
- Accent Embedding
Encoder的输入为带有纯净语言信息的`Bottleneck Feature`和`Accent ID`，输出为带有口音信息的隐变量`h`。

#### Decoder 
- Pre-Net
- Decoder-RNN
- Post-Net
- Speaker Embedding (i-vector, d-vector, x-vector)  

### Ablation Study
文中比较了三个模型：
- Baseline：标准普通话语音识别器+转换模型，没有说话者分类器作为判别器
- P1：基于口音的语音识别器+转换模型，没有说话者分类器作为判别器
- P2：基于口音的语音识别器+转换模型，有说话者分类器

## VQ-VAE模型


