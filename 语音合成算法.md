# 语音合成算法

## 1 End-to-End TTS Before Tacotron
- First step toward End-to-End Parametric TTS
  - 输入：phoneome 音素
  - 输出：acoustic feature for STRAIGHT (vocoder)
- Char2Wav
  - 输入：character
  - 输出：acoustic feature for SampleRNN (vocoder)
  

## 2 Tacotron
参考论文：[Tacotron: Towards End-to-End Speech Synthesis](https://arxiv.org/abs/1703.10135)  
来源：Interspeech 2017  
发表时间：2017.4.6  

Taco这个名字只是因为该论文的一部分作者喜欢吃Taco而得来，本来名字叫Talkotron。  
输入：character
输出：Linear/Mel-spectrogram
Tacotron网络框架  
Encoder + Attention + Decoder
![image](https://user-images.githubusercontent.com/40049927/136914526-1e0e4173-181f-4241-8e1b-fb0b9621282a.png)  

### Encoder = Grapheme-to-phoneme 字母→编码器→音素


### Attention
attention让模型自动学出每一个tone的embedding（encoder的输出）在decoder中能够产生多长的声音信号。

### Decoder
Decoder的结构跟`seq2seq model`的decoder结构类似，只是加入了`attention`的计算。  

作者使用了两种decoder：`attention RNN`和`output RNN`。`attention RNN`负责生成`query vector`作为`attention`模块的输入，`attention`模块生成`context vector`，最后`output RNN`将`query vector`和`context vector`组合在一起作为输入。  

每个time step，RNN输出`r`个vector，这些vector就是Mel-spectrogram。r=3或者5。因为声音信号非常长，每个mel vector 对应的时间太短了，可能只有0.01秒。对于几秒的语音，我们需要生成上百个vector，效率非常低下。一次产生3个，运算量能够减少三倍。但是在第二代`r=1`。然后将当前time step 输出的r个vector的最后一个作为下一个time step的输入。

#### 输出Mel-spectrogram还是Linear-spectrogram?  

最后RNN输出的是mel-spectrogram而不是spectrogram是出于计算量的原因。另外一个用来缩减计算量的做法是每个decoder step预测多个(r个)frame，且作者发现这样做还可以加速模型的收敛。  

在Decoder中使用`Teacher forcing`后中如何避免训练和测试的效果差异过大?**  
使用`teacher forcing`之后，在训练的时候，每个time step，Decoder输入的都是正确答案。在推理的时候，没有正确答案，可能会造成训练和测试的`mismatch`。对应的解决方案就是在`Pre-net`中加入`Dropout`，在训练过程中，`Dropout`就会模拟在测试时不理想的输入的情况，使得RNN能够正确地处理输入不理想的情况。  

#### 什么时候结束decode? 
添加了一个新的module，去判断是否需要结束，这是一个二分类器，输入是`Decoder RNN`的状态，输出这个状态是结束状态的概率。  

### CBHG
`CBHG`:(1-D convolution bank + highway network + bidirectional GRU)  
CBHG从序列中提取出高层次模块，一维卷积+highway+残差链接+双向GRU的组合，输入序列，输出同样也是序列。
 
### Pre-net
作者设计Pre-net的意图是让它成为一个bottleneck layer来提升模型的泛化能力，以加快收敛速度。Pre-net由全连接层+Dropout组成。

### Dropout
**在模型推理阶段，Dropout需要保留！！**。因为如果Droput去掉，模型在进行样本点生成的时候，会贪心地选择概率最大的样本点，这样会导致一些极其不自然的声音出现。  

### Post Processing
![image](https://user-images.githubusercontent.com/40049927/138563625-0aaf9a3e-ea89-4ecc-a89e-5e44d6609bad.png)  
`Post Processing`的输入是`Decoder RNN`产生的Mel-spectrogram序列，网络结构是由`CBHG`，输出是最终的Mel-spectrogram或者Linear-spectrogram。`Decoder RNN`输出的Mel-spectrogram是因果的，所以当位于前面的Mel-spectrogram不准确或者质量不高时，后面的Mel-spectrogram的质量可能因此被影响。加入后处理的网络的目的是**想去消除这种因果性，使得即使前面的Mel-spectrogram质量好坏不会影响到后面的spectrogram。

#### 两个loss，两个training targets  
1. `Decoder RNN`输出的vector就是Mel-spectrogram，即其输出与ground truth的Mel-spectrogram的距离需要最小化；
2. `CBHG`输出vector就是Mel-spectrogram，即其输出与ground truth的Mel-spectrogram的距离需要最小化；

## 3 Tacotron2
参考论文：[Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](https://arxiv.org/abs/1712.05884)  
来源：ICASSP 2018  
发表时间：2018.2.16  


**Tacotron2和Tacotron的主要不同在于**：  
- 不使用CBHG，而是使用普通的LSTM和Convolution layer
- decoder每一步只生成一个frame
- 增加post-net，即一个5层CNN来精调mel-spectrogram
- 使用Wavenet作为声码器
- 更新了Attention机制，Location-aware Attention

**WaveNet也需要训练**  
WaveNet训练的时候，应该用Tactron生成的acoustic featture作为输入，而不是真正的acoustic feature作为输入，这样分数会更高。

### Tacotron的缺陷
1. 推理速度太慢
2. 有时候会发音错误，原因在于训练集的词汇量不太够，对于没见过的单词，Tacotron不会念


## 语音合成的一些可控因素

### 基于语法的说话节奏优化
使得生成的声音能够有节奏感。

### Attention的优化
各种Attention的研究可以参考这篇论文：[Location-Relative Attention Mechanisms For Robust Long-Form Speech Synthesis](https://arxiv.org/abs/1910.10288)。  
另一篇关于Attention来自**百度**的第3代`DeepVoice`，来自论文[Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning](https://arxiv.org/abs/1710.07654)。在推理的时候，直接暴力地将超出对角线附近部分的`attention weight`直接置零。并且，对`attention`的`query`和`key`都加入了`position encoding`。而`positionnal encoding`是从`speaker embedding`生成的，其背后的原因是不同的speaker的说话速度和节奏不一样，所以相同的语句`position encoding`可能不同。  

`Monotonic Attention`要求Attention一定要由左向右，用在Tacotron1。  
`Guided Attention`直接限制Attention在对角线上，去惩罚那些偏离对角线的Attention。
`Location-aware Attention`


## 4 Transformer TTS
参考论文：[Neural Speech Synthesis with Transformer Network](https://arxiv.org/abs/1809.08895)  
来源：ICASSP 2018  
发表时间：2019.1.30  

## 5 FastSpeech - 基于Transformer的end-to-end TTS模型
参考论文：[FastSpeech: Fast, Robust and Controllable Text to Speech](https://arxiv.org/pdf/1905.09263.pdf)  
来源：ICASSP 2018  
发表时间：2019.1.30  

Tacotron2的decoder是一个自回归模型，合成速度慢。基于Transformer的FastSpeech在保持合成高质量语音的同时大幅提高了合成速度。

### Duration Model

#### 训练阶段

#### 推理阶段


## 6 FastSpeech 2
参考论文：[FastSpeech 2: Fast and High-Quality End-to-End Text to Speech](https://arxiv.org/abs/2006.04558)  
来源：ICLR 2021  
发表时间：2021.3.4  

FastSpeech存在着训练时间长等缺点。FastSpeech2改进了这些问题，使得模型的训练速度加快了3倍，且可以合成出音质比Tacotron更高的语音。


## DeepVoice
**百度**
[DeepVoice](https://arxiv.org/abs/1702.07825)  
[DeepVoice2](https://arxiv.org/abs/1705.08947)  
[DeepVoice3](https://arxiv.org/abs/1710.07654)  
